---
title: "Bigger data"
subtitle: "with `arrow` and `duckdb`"
author: "Tom Mock & Edgar Ruiz"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "css/my-theme.css", "css/my-fonts.css"]
    seal: false
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: dracula
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        eval = TRUE)
library(tidyverse)
library(testthat)
library(arrow)
library(duckdb)
library(DBI)
library(gt)
library(devtools)
library(usethis)
library(magrittr)
```

```{r metathis, echo=FALSE}
library(metathis)
meta() %>%
  meta_name("github-repo" = "jthomasmock/bigger-data") %>% 
  meta_social(
    title = "Bigger data with arrow and duckdb",
    description = paste(
      "Bigger than memory data is easy to work with in R,",
      "and can be amazingly fast with arrow or duckdb!"
    ),
    url = "https://jthomasmock.github.io/bigger-data",
    image = "https://raw.githubusercontent.com/jthomasmock/bigger-data/master/images/title-card.png",
    image_alt = paste(
      "Title slide of 'Bigger Data with arrow and duckdb'.", 
      "A slide deck covering the basics of using arrow and duckdb with bigger than memory data.",
      "It also has a picture of two people walking up a large incline/mountain."
    ),
    og_type = "website",
    og_author = "Tom Mock",
    twitter_card_type = "summary_large_image",
    twitter_creator = "@thomas_mock",
    twitter_site = "@thomas_mock"
  )
```

class: title-slide, left, top

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

### `r Sys.Date()`

<br>

`r fontawesome::fa("link", "white")` [colorado.rstudio.com/rsc/bigger-data-prez](https://colorado.rstudio.com/rsc/bigger-data-prez)   
`r fontawesome::fa("github", "white")` [github.com/jthomasmock/bigger-data-prez](https://github.com/jthomasmock/bigger-data-prez)  

<span style='color:white;'>Slides released under</span> [CC-BY 2.0](https://creativecommons.org/licenses/by/2.0/)&nbsp;&nbsp;`r fontawesome::fa("creative-commons", "white")``r fontawesome::fa("creative-commons-by", "white")` ]   

<div style = "position: absolute;top: 0px;right: 0;"><img src="https://images.unsplash.com/photo-1579538800945-46d13c694a36?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1470&q=80" alt="Two people walking up a large mountain" width="600"></img></div>

---

layout: true

<div class="my-footer"><span>https://colorado.rstudio.com/rsc/bigger-data-prez/</span></div>

---

### The initial ask

> * Some discussion around in-process analytics  
> * The concept and implementations like DuckDb and Arrow  
> * What the current landscape of options looks like, what is and isn't stable enough for enterprise use  
> * DuckDB backend for dplyr  

* [Hannes MÃ¼hleisen & Mark Raasveldt slides, 2021](https://homepages.cwi.nl/~hannes/sigmod2021-muehleisen-inprocess-slides.pdf)  
* [New directions for Apache Arrow, Wes McKinney 2021-09-13](https://www.slideshare.net/wesm/new-directions-for-apache-arrow)  
---

### Working with bigger data?

* Relational databases (IE SQL) are still around and hugely popular but...  

--

* Local files are getting bigger  

--

* Data Warehouses/Data Lakes often use flat-file storage (`.csv`, `.parquet`, `.json` etc)  

--

How do you deal with data that isn't already in a database, and is bigger than your memory?  

---

### Pause for one second

If it _can_ fit in memory, then try out:  

* [`vroom::vroom()`](https://vroom.r-lib.org/) or [`data.table::fread()`](https://rdatatable.gitlab.io/data.table/reference/fread.html) for fast file reads _into_ R  
  * [`vroom(col_select = c(column_name))`](https://vroom.r-lib.org/reference/vroom.html) also allows for partial reads (ie specific columns)  

--

* [`data.table`](https://rdatatable.gitlab.io/data.table/index.html) or [`dtplyr`](https://dtplyr.tidyverse.org/) for fast and efficient in-memory analysis  

--

* Lastly, the [`collapse`](https://sebkrantz.github.io/collapse/) R package for limited, but hyper-performant data manipulation  

---

### Two topics for today

Two relatively lightweight options:  

* The [Apache Arrow](https://arrow.apache.org/) project  

* [`duckdb`](https://duckdb.org/docs/api/r)  

--

* Also, `arrow` can be used inside [`sparklyr`](https://spark.rstudio.com/guides/arrow/) to great effect, but that brings into play Java, Scala, and `rJava` `r emo::ji("upside-down")`  

---

### `arrow`

> [Apache Arrow](https://arrow.apache.org/) is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication.

--

> The `arrow` package exposes an interface to the Arrow C++ library, enabling access to many of its features in R. It provides low-level access to the Arrow C++ library API and higher-level access through a `dplyr` backend and familiar R functions.  

--

For more info, checkout the slides from Wes McKinney as of [September 10th](https://www.slideshare.net/wesm/new-directions-for-apache-arrow)  




---

### [`duckdb`](https://duckdb.org/docs/why_duckdb.html)

> DuckDB plays well with `dbplyr` / `dplyr` for querying from R. - [duckdb.org/docs/api/r](https://duckdb.org/docs/api/r)  

--

#### Fast

> DuckDB is designed to support analytical query workloads, also known as Online analytical processing (OLAP). These workloads are characterized by complex, relatively long-running queries that process significant portions of the stored dataset, for example aggregations over entire tables.

--

#### Simple

> DuckDB has no external dependencies, neither for compilation nor during run-time [basically a 10x faster version of SQLite]  

--

#### Feature Rich

> Data can be stored in persistent, single-file databases. DuckDB is deeply integrated into Python and R for efficient interactive data analysis. Use[s] the PostgreSQL parser  

---

.pull-left[

### `r fontawesome::fa("check-circle", "#338333")` When to use DuckDB

* Processing and storing tabular datasets, e.g. from CSV or Parquet files  
* **Interactive data analysis**, e.g. Joining & aggregate multiple large tables  
* Concurrent large changes, to multiple large tables, e.g. appending rows, adding/removing/updating columns  
* Large result set transfer to client  

]

--


.pull-right[


### `r fontawesome::fa("times-circle", "red")` When to not use DuckDB

* Non-rectangular data sets, e.g. graphs, plaintext  
* High-volume transactional use cases (e.g. tracking orders in a webshop)  
* Large client/server installations for centralized enterprise data warehousing  
* Writing to a single database from multiple concurrent processes  

]

Credit: [duckdb.org/](https://duckdb.org/)

---

### `duckdb` + `dbplyr`

`duckdb` uses Postgres-flavored SQL, so it has deep integration with `dbplyr` out of the box.

--

```{r}
library("dplyr", warn.conflicts = FALSE)

con <- DBI::dbConnect(duckdb::duckdb()) # create a temp database in memory
duckdb::duckdb_register(con, "flights", nycflights13::flights)
tbl(con, "flights")
```

---

### `duckdb` + `dbplyr`

`duckdb` uses Postgres-flavored SQL, so it has deep integration with `dbplyr` out of the box.

--

```{r}
tbl(con, "flights") %>% 
  group_by(dest) %>%
  summarise(delay = mean(dep_time, na.rm = TRUE))
```


### How big is `nycflights13` anyway?

```{r}
lobstr::obj_size(nycflights13::flights) %>% 
  unclass() %>% 
  scales::label_bytes()(.)
```

--

### Save to disk as a `.csv`

```{r}
nycflights13::flights %>% 
  janitor::clean_names() %>% 
  write_delim("flights.csv", delim = ",")
```

--

```{r}
fs::file_info("flights.csv") %>% pull(size)
```


---

### Load into `duckdb` permanently 

```{r}
# write to disk as "flightDisk", other defaults to in memory
con <- DBI::dbConnect(duckdb::duckdb(), "flightDisk")

duckdb::duckdb_read_csv(conn = con, name = "flightsCSV",  files = "flights.csv",
                        header = TRUE, delim = ",", na.strings = "NA")
```

--

```{r}
DBI::dbListTables(con)
```

---

### Flying with the `duckdb`s

```{r, eval = FALSE}
flight_tbl <- tbl(con, "flightsCSV")
```

---

### Flying with the `duckdb`s

```{r}
flight_tbl <- tbl(con, "flightsCSV")
flight_tbl
```

---

### Flying with the `duckdb`s

```{r}
flight_tbl %>% 
  group_by(month, origin) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE), .groups = "drop") %>% 
  arrange(desc(avg_delay)) # all on disk still
```

---

### Landing the `duckdb` in memory

.pull-left[

```{r duckdbPlot, eval = FALSE}
flight_tbl %>% 
  group_by(origin, month, day) %>% 
  summarise(
    avg_delay = mean(dep_delay, na.rm = T), 
    .groups = "drop"
    ) %>% 
  arrange(desc(avg_delay)) %>% 
  # collect() to bring into R
  collect() %>% 
  # and then it's like any other dataframe!
  ggplot(aes(x = month, y = avg_delay)) +
  geom_boxplot(aes(group = month)) +
  geom_jitter(
    aes(color = origin), 
    alpha = 0.2, width = 0.4) +
  facet_wrap(~origin, ncol = 1)
```

]

--

.pull-right[

```{r duckdbPlot, eval = TRUE, echo = FALSE}
```


]

---

### `arrow` and `duckdb` integration

```{r}
arrow::InMemoryDataset$create(mtcars) %>%
  filter(mpg < 30) %>%
  arrow::to_duckdb() %>% # oneliner to move into duckdb
  group_by(cyl) %>%
  summarize(mean_mpg = mean(mpg, na.rm = TRUE))
```

---

### Big data

ok... so `mtcars` is most definitely not big data.

--

And ~30 Mb of `nycflights.csv` is not that impressive. 

--

Let's use the `arrow` example data, stay in NYC, and look at the `nyc-taxi` fare dataset

--

```{r fareAndSize}
nyc_fares <- fs::dir_info("nyc-taxi", recurse = TRUE) %>%
  filter(type == "file") %>% 
  summarise(n = n(), size = sum(size)) 

glue::glue("There are {nyc_fares$n} files, totaling {nyc_fares$size}!")
```

--

Now that we have a full quiver of data to pull from, let's shoot some `arrow`s

--

Also RIP my laptop's hard drive space `r emo::ji("bomb")` `r emo::ji("boom")` `r emo::ji("bomb")`

---

### Shoot some `arrow`s

Benchmarked on a Macbook Pro 2017 with a SSD

```{r startArrow}
library(duckdb)
library(pins)
library(arrow, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tictoc) # for timing

# arrow::arrow_with_s3()
# setwd("/Volumes/mock-external/")
# arrow::copy_files("s3://ursa-labs-taxi-data", "nyc-taxi")
# warning that downloads 37Gb of data!


tic()
ds <- open_dataset("nyc-taxi", partitioning = c("year", "month"))
toc() # just to show that this is a fast process
```

---

### How many ar**rows**?

```{r ludicrousSpeed, cache = TRUE}
tic()
full_collect <- summarise(ds, n = n()) %>% 
  collect() %>% 
  pull(n)
n_rows <- scales::unit_format(unit = "billion", scale = 1e-9, 
                      accuracy = 0.01)(full_collect)
glue::glue("There are approximately {n_rows} rows!")
toc() # wow that's fast.
```


---

### Ludicrous speed!

<iframe src="https://giphy.com/embed/izspP6uMbMeti" width="480" height="254" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/community-playstation-xo-izspP6uMbMeti">via GIPHY</a></p>

---

### How can you query 1.5 BILLION rows in a few seconds?

`arrow` is a columnar data format, as opposed to more traditional storage that is row-based.

--

`arrow` provides an R interface to `feather` and `parquet`, along with more traditional formats. It also provides a translation of `dplyr` into `dbplyr`-style queries OUT of memory.

--

With row-based, when `filter()` or `select()`, the query engine would have to scan every row, parse each column, extract the specified matches and then perform calculations

--

With columnar formats, you can completely skip unnecessary columns, and even for a relatively "narrow" dataset of 6x columns, this means 1/6th the processing... or essentially a 600% performance gain. You can imagine the performance gains when you have wide data with dozens of additional columns that are only needed intermittently.

---

### How can you query 1.5 BILLION rows in a few seconds?

---

### How can you query ~~1.5 BILLION rows~~ in a few seconds?

The answer? By not reading in 1.5 billion rows.

--

`arrow`'s magic, is that `filter()` + `select()` + columnar data format are "cheat codes" for ludicrously fast queries. BUT it's also very optimized, and simply reading in entire files is also very fast. [Neal Richardson, NYR 2020](https://enpiar.com/talks/nyr-2020/#41)

--

```{r NYRSlides, out.width="50%", echo = FALSE}
knitr::include_graphics("https://enpiar.com/talks/nyr-2020/img/taxi-single-1.png")
```

---

### One more piece of "magic"

`arrow` and `arrow::open_dataset()` allows for the creation and reading of partitioned datasets. The `nyc-taxi` data is _actually_ 126 individual parquet files. Each file is about 300 Mb.

--

```{r}
ds <- open_dataset("nyc-taxi", partitioning = c("year", "month"))
```


--

```{r}
fs::dir_ls("nyc-taxi/", recurse = TRUE) %>%  
  stringr::str_subset("parquet", negate = TRUE) %>% 
  stringr::str_subset("\\/20[0-9]+\\/") %T>% 
  {cat("There are", length(.), "files\n")} %>% return()
```


---

### Big data, new data

.left-wide[
```{r, eval = FALSE}
tic()
ds %>%
  filter(total_amount > 100, year == 2015) %>%
  select(tip_amount, total_amount, passenger_count) %>%
  # calculate a new column, on disk!
  mutate(tip_pct = 100 * tip_amount / total_amount) %>%
  group_by(passenger_count) %>%
  summarise(
    mean_tip_pct = mean(tip_pct),
    n = n()
  ) %>%
  collect() %>%
  print()
toc()
```

]

--

.right-narrow[
```{r, cache = TRUE, echo = FALSE}
tic()
ds %>%
  filter(total_amount > 100, year == 2015) %>%
  select(tip_amount, total_amount, passenger_count) %>%
  # calculate a new column, on disk!
  mutate(tip_pct = 100 * tip_amount / total_amount) %>%
  group_by(passenger_count) %>%
  summarise(
    mean_tip_pct = mean(tip_pct),
    n = n()
  ) %>%
  collect() %>%
  print()
toc()
```

]

---

### Big data, all the rows

OK not all the rows, but at least the rows with actual passengers (theres some negative values in `passenger_count`... `r emo::ji("thinking")`)

--
.left-wide[
```{r, eval = FALSE}
tic()
ds %>% 
  select(passenger_count, total_amount) %>% 
  filter(between(passenger_count, 0, 6)) %>% 
  group_by(passenger_count) %>% 
  summarise(
    n = n(),
    mean_total = mean(total_amount, na.rm = TRUE)
    ) %>% 
  collect() %>% # pull into memory!
  arrange(desc(passenger_count))
toc()
```


]

--

.right-narrow[
```{r, echo = FALSE, cache=TRUE}
tic()
ds %>% 
  select(passenger_count, total_amount) %>% 
  filter(between(passenger_count, 0, 6)) %>% 
  group_by(passenger_count) %>% 
  summarise(n = n(),
            mean_total = mean(total_amount, na.rm = TRUE)) %>% 
  collect() %>% # pull into memory!
  arrange(desc(passenger_count))
toc()
```

]

---

### Big data, elsewhere

Same data, but on a remote hard drive, 5400 RPM, via USB 2.0.

.pull-left[

```{r}
ds_external <- open_dataset(
  "/Volumes/mock-external/nyc-taxi", 
  partitioning = c("year", "month"))
```

```{r remoteData, eval = FALSE}
tic()
ds_external %>%
  filter(total_amount > 100, year == 2015) %>%
  select(tip_amount, total_amount, passenger_count) %>%
  # calculate a new column, on remote disk
  mutate(tip_pct = 100 * tip_amount / total_amount) %>%
  group_by(passenger_count) %>%
  summarise(mean_tip_pct = mean(tip_pct),
    n = n()) %>% collect()
toc()
```


]

--

.pull-right[

```{r remoteData, eval = TRUE, echo=FALSE, cache = TRUE}
```

]

---

### We can shoot ducks with arrows

Going back to local SSD and using `arrow::to_duckdb()`, full `group_by()`/`mutate()` support!

--

.left-wide[
```{r, eval = FALSE}
tic()
ds %>%
  filter(total_amount > 100, year == 2015) %>%
  select(tip_amount, total_amount, passenger_count) %>%
  # use arrow to populate directly into a duckdb
  arrow::to_duckdb() %>% 
  group_by(passenger_count) %>%  # group_by mutate!
  mutate(tip_pct = 100 * tip_amount / total_amount) %>%
  filter(tip_pct >= 25) %>% 
  summarise(n = n()) %>% collect()
toc()
```

]

--

.right-narrow[
```{r, echo = FALSE, cache = TRUE}
tic()
ds %>%
  filter(total_amount > 100, year == 2015) %>%
  select(tip_amount, total_amount, passenger_count) %>%
  # use arrow to populate directly into a duckdb
  arrow::to_duckdb() %>% 
  group_by(passenger_count) %>%  # group_by mutate!
  mutate(tip_pct = 100 * tip_amount / total_amount) %>%
  filter(tip_pct >= 25) %>% 
  summarise(n = n()) %>% collect()
toc()
```
]

---

### `pins`

Now, as far as using `arrow` or `duckdb`, since they are single files, you _could_ `pin` them

--

Note, `pins` 1.0, releasing in next few days has native support for `feather` which is essentially an `arrow` dataframe on disk.

--

`duckdb` is a single file, but you should just use `feather` anyway as it's fast to read, relatively efficient to store, and allows you to mix `arrow` native work along with `arrow::to_duckdb()` if you need `duckdb` querying.

--

You could also go `parquet` and switch from `pin_write()` to `pin_upload()`

---

### `pins` comparison

.pull-left[

```{r, eval = FALSE}
query_feather <- function(){
  tic()
  feather_file <- pin_download(
    board_rsc, 
    "thomas/big-nyc-feather")
  
  feather_file <- arrow::open_dataset(
    feather_file, format = "feather")
  
  summary_feather <- feather_file %>% 
    select(fare_amount, vendor_id) %>% 
    filter(vendor_id == "1") %>% 
    summarise(mean = mean(fare_amount)) %>% 
    collect()

  toc()
}
```


]

--

.pull-right[

```{r, eval = FALSE}
query_rds <- function(){
  
  tic()
  
  pinned_rds <- pin_read(
    board_rsc, 
    "thomas/big-nyc-rds")
  
  summary_rds <- pinned_rds %>% 
    select(fare_amount, vendor_id) %>% 
    filter(vendor_id == "1") %>% 
    summarise(mean = mean(fare_amount))
  
  toc()
}
```

]


---

### `pins` comparison

`feather` is native to arrow, so it allows for rapid file reading AND allows for partial reading, through a combination of `filter()` + `select()` + `summarize()`

--

So, `pins` + `feather` + `arrow::open_dataset()` allows for insanely fast queries on relatively large datasets.

--

```{r, eval = FALSE}
query_feather()
#> 12.4 sec elapsed
query_rds()
#> 12.4 sec elapsed
```

--

I promised you speed, but they're the same! You can't "beat" download times `r emo::ji("shrug")`


---

### `pins` comparison

While you can't beat file transfer, disk space is infinitely cheaper than memory...

--

And your downloaded file still exists!

```{r}
file.exists("~/Library/Caches/pins/rsc-e62371cfd77db754024f9c5ed3556a73/2f2ee2e3-564a-4791-a7f3-01e22ad2939f/48597/big-nyc-feather.arrow")
```

--

```{r, eval = FALSE}
query_feather()
#> 1.3 sec elapsed
query_rds()
#> 13.8 sec elapsed
```

---

### `pins` comparison

Now, you could get similar speed ups by loading the data completely into memory 1x, but now you're lugging around at least 1 Gb of data for each session... and there are _very few times_ when a user **needs** 1 Gb of data.

--

By selecting "windows" of data rapidly, you get the ability to perform useful queries/summaries across large swaths of data with minimal memory load, and minimal read time.

---

### `pins` comparison

So, in your `shiny` apps:

--

```{r, eval = FALSE}
feather_file <- pin_download(board_rsc, "thomas/big-nyc-feather")

feather_ds <- arrow::open_dataset(feather_file, format = "feather")

server <- function(input, output, session) {
  # Lots more code
  feather_ds %>% 
    some_queries %>% 
    collect() %>% 
    if(type == "plot"){
      plot_it()
    } else if(type == "table"){
      table_it()
    } else {
      work_it() %>% make_it() %>% do_it() %>% makes_us() %>% 
        harder_better_faster_stronger()
    }
}
```

---

### `pins` + `duckdb`

File still exists!

--

```{r, eval = FALSE}
tic()
feather_file <- pin_download(board_rsc, "thomas/big-nyc-feather")

feather_file <- arrow::open_dataset(feather_file, format = "feather")

summary_duck <- feather_file %>%
  select(fare_amount, vendor_id) %>%
  filter(vendor_id == "1") %>%
  arrow::to_duckdb() %>%
  summarise(mean = mean(fare_amount, na.rm = TRUE)) %>%
  collect()
toc()
#> 1.48 sec
```


---

### Keeping your `pins` at a distance

If your team happens to use AWS, you can make use of [native S3 support with `arrow`](https://arrow.apache.org/docs/r/articles/fs.html)  

--

Similar to our `pins` workflow, there is unavoidable file-transfer time BUT you can read the files remotely and you can partition the data in S3.

--

This allows `arrow` to intelligently transfer/read only portions of the partitions as necessary, optimizing both the file transfer AND the query/read.

---

### Things we didn't talk about

* Snowflake (uses `arrow` under the hood)  

--

* Dremio (uses `arrow` under the hood)  

--

* Rinse and repeat across many other databases/data warehouse engines, as `arrow` is quickly becoming a standard tool for lots of database/data warehouse/data lake providers  

--

The core idea is if you have a "real" data warehouse, your team has likely already invested in some way to perform queries (and you can probably just use ODBC or equivalent).

--

BUT if you have a need to work with large flat-files, `arrow` and/or `duckdb` are very attractive tools.

--

`pins` can be leveraged along with `arrow`, but you lose a lot of the power of partitioning, since it's a single file. That being said, with `shiny` app-level data, you should be more than fine with a single file.
